{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A notebook to support academic question exploration and literature search / what do you do when you search the literature?\n",
    "\n",
    "0. Define a research question e.g Is occupational **asbestos exposure** an underecognised **cause** of IPF? \n",
    "1. Consider the different possible ways of answering the question (methods). Different study designs and ways of measuring asbestos exposure e.g Epidemiological, observational, cross-sectional, cohort, case-control, post-mortem and explant studies, ecological, toxicology, animal models, molecular disease models, exposure assessment, occupational hygeinst measurements, minerologic analysis (tissue, BAL etc)\n",
    "2. Generate search terms e.g \"IPF\", \"case-control\", \"occupational\", \"asbestos\" (? && mesh terms)\n",
    "3. Carry out search using search terms and e.g pubmed, google scholar, scopus, biorxiv, web of science, clinicaltrials.gov, ?google books\n",
    "4. Search results == Candidate Papers\n",
    "5. Extract title | journal | author | location | year | abstract | key words | full text && save result (as .bib) (prob want to export to jabref)\n",
    "6. Review Candidate Papers to identify Relevant Papers \n",
    "7. Use Relevant Papers to identify more Candidate papers. Search also by author, cited by, cite, [triangle closing](https://en.wikipedia.org/wiki/Triadic_closure) e.g https://github.com/hinnefe2/bibcheck.py and other means (?tensorflow)\n",
    "8. Use the Relevant Papers collected for whatever it is they are relevant for (usually to help compose a written document in which they are cited)\n",
    "\n",
    "meta: github/stack exchange etc to check out other peoples search strategies. this is likely to be formulated as a machine learning problem somewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### interesting related I found includes: https://www.projectcredo.com/, http://citationexplorer.hoppmann.me/, lict from a previous nhshackday, https://github.com/jvoytek/pubmedbrain/blob/f5170a2e3540e0c2aa665559c86048dfb1583f16/documents/Voytek-brainSCANrPreprint.pdf, https://github.com/graeham/hackathon/blob/master/paperGraph.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search github for relevant stuff with the following 'webbit' \n",
    "> https://github.com/search?l=Python&q=http%3A%2F%2Feutils.ncbi.nlm.nih.gov%2Fentrez%2Feutils%2Fesearch.fcgi++stars%3A%3E5&ref=advsearch&type=Code&utf8=%E2%9C%93\n",
    "\n",
    "gists and interwebs inc stackoverflow also helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tempting to dive into django a la https://github.com/afouchet/OpenReview but probably not essential and now is not optimal timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/gui11aume looks well documented, poss useful template\n",
    "https://github.com/swcarpentry/2013-08-23-harvard/blob/b2097bc20833e0a58b2e73eecd1227d61bd5a00a/lessons/misc-biopython/eutils.md looks like nice intro to biopython utils and https://gist.github.com/bonzanini/5a4c39e4c02502a8451d, https://gist.github.com/ehazlett/1104507, https://gist.github.com/vtrubets/ef1dabb397ea6a05ce5b4e767ed15af9 (for use of icite), https://gist.github.com/mcfrank/c1ec74df1427278cbe53, http://stackoverflow.com/questions/17409107/obtaining-data-from-pubmed-using-python, https://github.com/bwallace/abstrackr-web/tree/master/abstrackr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's tackle pubmed first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNotebook to support academic question exploration and literature search.\\n\\nThanks to https://marcobonzanini.wordpress.com/2015/01/12/searching-pubmed-with-python/ and \\nhttp://www.fredtrotter.com/2014/11/14/hacking-on-the-pubmed-api/\\n\\nPubmed advanced search is helpful for designing search/experimenting https://www.ncbi.nlm.nih.gov/pubmed/advanced\\n\\nDocs for NCBI esearch:\\nhttps://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\\nhttps://www.nlm.nih.gov/bsd/mms/medlineelements.html\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Notebook to support academic question exploration and literature search.\n",
    "\n",
    "Thanks to https://marcobonzanini.wordpress.com/2015/01/12/searching-pubmed-with-python/ and \n",
    "http://www.fredtrotter.com/2014/11/14/hacking-on-the-pubmed-api/\n",
    "\n",
    "Pubmed advanced search is helpful for designing search/experimenting https://www.ncbi.nlm.nih.gov/pubmed/advanced\n",
    "\n",
    "Docs for NCBI esearch:\n",
    "https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
    "https://www.nlm.nih.gov/bsd/mms/medlineelements.html\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from Bio import Medline\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_chunked_pmids(term, chunksize=50):\n",
    "    \"\"\"\n",
    "    Return a list of Pubmed ids from pubmed search in chunks\n",
    "    \"\"\"\n",
    "    Entrez.email = \"carl.reynolds@imperial.ac.uk\"\n",
    "    count_handle = Entrez.esearch(db=\"pubmed\",\n",
    "                                  term=term,\n",
    "                                  sort=\"relevance\",\n",
    "                                  retmode=\"xml\",\n",
    "                                  rettype=\"count\")\n",
    "    count_results = Entrez.read(count_handle)\n",
    "    count = int(count_results[\"Count\"])\n",
    "\n",
    "    retmax_requests = list(range(chunksize, count, chunksize))\n",
    "    retmax_requests.append(count - retmax_requests[len(retmax_requests) - 1])\n",
    "\n",
    "    for i, retmax in enumerate(retmax_requests):\n",
    "        pmid_handle = Entrez.esearch(db=\"pubmed\",\n",
    "                                     term=term,\n",
    "                                     sort=\"relevance\",\n",
    "                                     retmode=\"xml\",\n",
    "                                     usehistory='y',\n",
    "                                     retstart=retmax,\n",
    "                                     retmax=chunksize)\n",
    "        results = Entrez.read(pmid_handle)\n",
    "        yield results[\"IdList\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pubmed_summary(pubmed_id):\n",
    "    \"\"\"\n",
    "    Use the Pubmed API to return the summary of a pubmed article\n",
    "    \"\"\"\n",
    "    Entrez.email = \"carl.reynolds@imperial.ac.uk\"\n",
    "    pubmed_id = ', '.join(map(str, pubmed_id))\n",
    "    handle = Entrez.esummary(db='pubmed', \n",
    "                             id=pubmed_id, \n",
    "                             retmode='json', \n",
    "                             rettype='abstract')\n",
    "    return json.loads(handle.read())['result']\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pubmed_abstract(pubmed_id):\n",
    "    \"\"\"\n",
    "    Use the Pubmed API to return the abstract of a pubmed article\n",
    "    \"\"\"\n",
    "    Entrez.email = \"carl.reynolds@imperial.ac.uk\"\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           id=pubmed_id,\n",
    "                           retmode='text',\n",
    "                           rettype='abstract')\n",
    "    return handle.read()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pubmed_keywords(pubmed_id):\n",
    "    \"\"\"\n",
    "    Use the Pubmed API to return the medline record and extract the key words of a pubmed article\n",
    "    \"\"\"\n",
    "    Entrez.email = \"carl.reynolds@imperial.ac.uk\"\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           id=pubmed_id,\n",
    "                           rettype='medline',\n",
    "                           retmode='text')\n",
    "    records = Medline.parse(handle)\n",
    "    keywords = []\n",
    "    for record in records:\n",
    "        mh = record.get('MH','?')\n",
    "        for w in mh: \n",
    "            if w not in keywords:\n",
    "                keywords.append(w)\n",
    "        keywords.sort()\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_citation_information(pubmed_id):\n",
    "    \"\"\"\n",
    "    Use the special citation api to return relative citation ratios\n",
    "    \"\"\"\n",
    "    citation_search = 'https://icite.od.nih.gov/api/pubs/{0}'.format(pubmed_id)\n",
    "    response = requests.get(citation_search).content\n",
    "    str_response = response.decode('utf-8')\n",
    "    try:\n",
    "        return json.loads(str_response)['relative_citation_ratio']\n",
    "    except KeyError:\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed block 0\n",
      "Processed block 1\n",
      "Processed block 2\n",
      "Processed block 3\n",
      "Processed block 4\n",
      "Processed block 5\n",
      "Processed block 6\n",
      "Processed block 7\n",
      "Processed block 8\n",
      "Processed block 9\n",
      "Processed block 10\n",
      "Processed block 11\n",
      "Processed block 12\n",
      "Processed block 13\n",
      "Processed block 14\n",
      "Processed block 15\n",
      "Processed block 16\n",
      "Processed block 17\n",
      "Processed block 18\n",
      "Processed block 19\n",
      "Processed block 20\n",
      "Processed block 21\n",
      "Processed block 22\n",
      "Processed block 23\n",
      "Processed block 24\n",
      "Processed block 25\n",
      "Processed block 26\n",
      "Processed block 27\n",
      "Processed block 28\n",
      "Processed block 29\n",
      "Processed block 30\n",
      "Processed block 31\n",
      "Processed block 32\n",
      "Processed block 33\n"
     ]
    }
   ],
   "source": [
    "def lit_search(term):\n",
    "    \"\"\"\n",
    "    Search pubmed for a term and collect information about the results\n",
    "    \"\"\"\n",
    "    pmid_blocks = get_chunked_pmids(term, chunksize=200)\n",
    "    summaries = []\n",
    "    abstracts = []\n",
    "    keywords = []\n",
    "    rcrs = []\n",
    "    litsearch_results = [summaries, abstracts, keywords, rcrs]\n",
    "    for i, block in enumerate(pmid_blocks):\n",
    "        summaries.append(get_pubmed_summary(block))\n",
    "        abstracts.append(get_pubmed_keywords(block))\n",
    "        keywords.append(get_pubmed_keywords(block))\n",
    "        rcrs.append(get_citation_information(block))\n",
    "        print(\"Processed block {0}\".format(i))\n",
    "    pickle.dump( litsearch_results, open( \"litsearch_results{0}.p\".format(term), \"wb\" ) )\n",
    "    return litsearch_results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term = 'idiopathic pulmonary fibrosis'\n",
    "list_of_lists = [summaries, abstracts, keywords, rcrs]\n",
    "pickle.dump( list_of_lists, open( \"list_of_lists_{0}.p\".format(term), \"wb\" ) )\n",
    "#can make resume?\n",
    "#can support export to .bib?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term = 'idiopathic pulmonary fibrosis'\n",
    "list_of_lists = pickle.load( open( \"list_of_lists_{0}.p\".format(term), \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#records = pickle.load( open( \"records.p\", \"rb\" ) )\n",
    "# write update function\n",
    "# add pub type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-a85dcb679feb>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-a85dcb679feb>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    df['title'] =\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for i, item in enumerate(list_of_lists):\n",
    "    for i, item in enumerate(list_of_lists[i]):\n",
    "         for i, item in enumerate(list_of_lists[i]):\n",
    "                df['title'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pubmed_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3155bda24e0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpubmed_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pmid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pmid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msummaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'firstauthor'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pmid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msummaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sortfirstauthor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lastauthor'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pmid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msummaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lastauthor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'journal'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pmid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msummaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pubmed_ids' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(pubmed_ids, columns=['pmid'])\n",
    "df['title'] = df['pmid'].map(lambda x: summaries.get(x)['title'])\n",
    "df['firstauthor'] = df['pmid'].map(lambda x: summaries.get(x)['sortfirstauthor'])\n",
    "df['lastauthor'] = df['pmid'].map(lambda x: summaries.get(x)['lastauthor'])\n",
    "df['journal'] = df['pmid'].map(lambda x: summaries.get(x)['source'])\n",
    "df['pubdate'] = df['pmid'].map(lambda x: summaries.get(x)['sortpubdate'])\n",
    "df['keywords'] = df['pmid'].map(lambda x: keywords.get(x))\n",
    "df['rcr'] = df['pmid'].map(lambda x: rcrs.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',300)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(list_of_dicts)\n",
    "df2.transpose().head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

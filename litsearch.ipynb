{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A notebook to support academic question exploration and literature search / what do you do when you search the literature?\n",
    "\n",
    "0. Define a research question e.g Is occupational **asbestos exposure** an underecognised **cause** of IPF? \n",
    "1. Consider the different possible ways of answering the question (methods). Different study designs and ways of measuring asbestos exposure e.g Epidemiological, observational, cross-sectional, cohort, case-control, post-mortem and explant studies, ecological, toxicology, animal models, molecular disease models, exposure assessment, occupational hygeinst measurements, minerologic analysis (tissue, BAL etc)\n",
    "2. Generate search terms e.g \"IPF\", \"case-control\", \"occupational\", \"asbestos\" (? && mesh terms)\n",
    "3. Carry out search using search terms and e.g pubmed, google scholar, scopus, biorxiv, web of science, clinicaltrials.gov, ?google books\n",
    "4. Search results == Candidate Papers\n",
    "5. Extract title | journal | author | location | year | abstract | key words | full text && save result (as .bib) (prob want to export to jabref)\n",
    "6. Review Candidate Papers to identify Relevant Papers \n",
    "7. Use Relevant Papers to identify more Candidate papers. Search also by author, cited by, cite, [triangle closing](https://en.wikipedia.org/wiki/Triadic_closure) e.g https://github.com/hinnefe2/bibcheck.py and other means (?tensorflow)\n",
    "8. Use the Relevant Papers collected for whatever it is they are relevant for (usually to help compose a written document in which they are cited)\n",
    "\n",
    "meta: github/stack exchange etc to check out other peoples search strategies. this is likely to be formulated as a machine learning problem somewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### interesting related I found includes: https://www.projectcredo.com/, http://citationexplorer.hoppmann.me/, lict from a previous nhshackday, https://github.com/jvoytek/pubmedbrain/blob/f5170a2e3540e0c2aa665559c86048dfb1583f16/documents/Voytek-brainSCANrPreprint.pdf, https://github.com/graeham/hackathon/blob/master/paperGraph.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search github for relevant stuff with the following 'webbit' \n",
    "> https://github.com/search?l=Python&q=http%3A%2F%2Feutils.ncbi.nlm.nih.gov%2Fentrez%2Feutils%2Fesearch.fcgi++stars%3A%3E5&ref=advsearch&type=Code&utf8=%E2%9C%93\n",
    "\n",
    "gists and interwebs inc stackoverflow also helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tempting to dive into django a la https://github.com/afouchet/OpenReview but probably not essential and now is not optimal timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/gui11aume looks well documented, poss useful template\n",
    "https://github.com/swcarpentry/2013-08-23-harvard/blob/b2097bc20833e0a58b2e73eecd1227d61bd5a00a/lessons/misc-biopython/eutils.md looks like nice intro to biopython utils and https://gist.github.com/bonzanini/5a4c39e4c02502a8451d, https://gist.github.com/ehazlett/1104507, https://gist.github.com/vtrubets/ef1dabb397ea6a05ce5b4e767ed15af9 (for use of icite), https://gist.github.com/mcfrank/c1ec74df1427278cbe53, http://stackoverflow.com/questions/17409107/obtaining-data-from-pubmed-using-python, https://github.com/bwallace/abstrackr-web/tree/master/abstrackr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's tackle pubmed first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNotebook to support academic question exploration and literature search.\\n\\nThanks to https://marcobonzanini.wordpress.com/2015/01/12/searching-pubmed-with-python/ and \\nhttp://www.fredtrotter.com/2014/11/14/hacking-on-the-pubmed-api/\\n\\nPubmed advanced search is helpful for designing search/experimenting https://www.ncbi.nlm.nih.gov/pubmed/advanced\\n\\nDocs for NCBI esearch:\\nhttps://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\\nhttps://www.nlm.nih.gov/bsd/mms/medlineelements.html\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Notebook to support academic question exploration and literature search.\n",
    "\n",
    "Thanks to https://marcobonzanini.wordpress.com/2015/01/12/searching-pubmed-with-python/ and \n",
    "http://www.fredtrotter.com/2014/11/14/hacking-on-the-pubmed-api/\n",
    "\n",
    "Pubmed advanced search is helpful for designing search/experimenting https://www.ncbi.nlm.nih.gov/pubmed/advanced\n",
    "\n",
    "Docs for NCBI esearch:\n",
    "https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
    "https://www.nlm.nih.gov/bsd/mms/medlineelements.html\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from Bio import Medline\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_chunked_pmids(term, chunksize=50):\n",
    "    \"\"\"\n",
    "    Return a list of Pubmed ids from pubmed search in chunks\n",
    "    \"\"\"\n",
    "    Entrez.email = \"carl.reynolds@imperial.ac.uk\"\n",
    "    count_handle = Entrez.esearch(db=\"pubmed\",\n",
    "                                  term=term,\n",
    "                                  retmode=\"xml\",\n",
    "                                  rettype=\"count\")\n",
    "    count_results = Entrez.read(count_handle)\n",
    "    count = int(count_results[\"Count\"])\n",
    "\n",
    "    retmax_requests = list(range(chunksize, count, chunksize))\n",
    "    retmax_requests.append(count - retmax_requests[len(retmax_requests) - 1])\n",
    "\n",
    "    for i, retmax in enumerate(retmax_requests):\n",
    "        pmid_handle = Entrez.esearch(db=\"pubmed\",\n",
    "                                     term=term,\n",
    "                                     sort=\"relevance\",\n",
    "                                     retmode=\"xml\",\n",
    "                                     usehistory='y',\n",
    "                                     retstart=retmax,\n",
    "                                     retmax=chunksize)\n",
    "        results = Entrez.read(pmid_handle)\n",
    "        yield results[\"IdList\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pubmed_summaries(pubmed_id):\n",
    "    \"\"\"\n",
    "    Use the Pubmed API to return the summary of a pubmed article\n",
    "    \"\"\"\n",
    "    Entrez.email = \"carl.reynolds@imperial.ac.uk\"\n",
    "    pubmed_id = ', '.join(map(str, pubmed_id))\n",
    "    handle = Entrez.esummary(db='pubmed', \n",
    "                             id=pubmed_id, \n",
    "                             retmode='json', \n",
    "                             rettype='abstract')\n",
    "    return json.loads(handle.read())['result']\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pubmed_keywords(pubmed_id):\n",
    "    \"\"\"\n",
    "    Use the Pubmed API to return the medline record and extract the key words of a pubmed article.\n",
    "    Return {pmid:[list of keywords]}.\n",
    "    \"\"\"\n",
    "    Entrez.email = \"carl.reynolds@imperial.ac.uk\"\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           id=pubmed_id,\n",
    "                           rettype='medline',\n",
    "                           retmode='text')\n",
    "    records = Medline.parse(handle)\n",
    "    kw = []\n",
    "    keywords = {}\n",
    "    for record in records:\n",
    "        pmid = record.get('PMID','?')\n",
    "        mh = record.get('MH','?')\n",
    "        for w in mh: \n",
    "            if w not in kw:\n",
    "                kw.append(w)\n",
    "        kw.sort()\n",
    "        keywords[pmid] = kw\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pubmed_abstracts(pubmed_id):\n",
    "    \"\"\"\n",
    "    Use the Pubmed API to return the medline record and extract the abstract of a pubmed article.\n",
    "    Return {pmid:abstract}.\n",
    "    \"\"\"\n",
    "    Entrez.email = \"carl.reynolds@imperial.ac.uk\"\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           id=pubmed_id,\n",
    "                           rettype='medline',\n",
    "                           retmode='text')\n",
    "    records = Medline.parse(handle)\n",
    "    abstracts = {}\n",
    "    for record in records:\n",
    "        pmid = record.get('PMID','?')\n",
    "        ab = record.get('AB','?')\n",
    "        abstracts[pmid] = ab\n",
    "    return abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pubmed_pubtypes(pubmed_id):\n",
    "    \"\"\"\n",
    "    Use the Pubmed API to return the medline record and extract the abstract of a pubmed article.\n",
    "    Return {pmid:pubtype}.\n",
    "    \"\"\"\n",
    "    Entrez.email = \"carl.reynolds@imperial.ac.uk\"\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           id=pubmed_id,\n",
    "                           rettype='medline',\n",
    "                           retmode='text')\n",
    "    records = Medline.parse(handle)\n",
    "    pubtypes = {}\n",
    "    for record in records:\n",
    "        pmid = record.get('PMID','?')\n",
    "        pt = record.get('PT','?')\n",
    "        pubtypes[pmid] = pt\n",
    "    return pubtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_citation_information(pubmed_id):\n",
    "    \"\"\"\n",
    "    Use the special citation api to return relative citation ratios\n",
    "    \"\"\"\n",
    "    pubmed_id = ','.join(pubmed_id)\n",
    "    citation_search = 'https://icite.od.nih.gov/api/pubs?pmids={0}'.format(pubmed_id)\n",
    "    response = requests.get(citation_search).content\n",
    "    str_response = response.decode('utf-8')\n",
    "    data = json.loads(str_response)['data']\n",
    "    citations = {}\n",
    "    for record in data:\n",
    "        pmid = record.get('pmid')\n",
    "        rcr = record.get('relative_citation_ratio')\n",
    "        citations[pmid] = rcr\n",
    "    return citations\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lit_search(term):\n",
    "    \"\"\"\n",
    "    Search pubmed for a term and collect information about the results\n",
    "    \"\"\"\n",
    "    pmid_blocks = get_chunked_pmids(term, chunksize=200)\n",
    "    summaries = []\n",
    "    pubtypes = []\n",
    "    abstracts = []\n",
    "    keywords = []\n",
    "    rcrs = []\n",
    "    litsearch_results = [summaries, pubtypes, abstracts, keywords, rcrs]\n",
    "    for i, block in enumerate(pmid_blocks):\n",
    "        summaries.append(get_pubmed_summaries(block))\n",
    "        pubtypes.append(get_pubmed_pubtypes(block))\n",
    "        abstracts.append(get_pubmed_abstracts(block))\n",
    "        keywords.append(get_pubmed_keywords(block))\n",
    "        rcrs.append(get_citation_information(block))\n",
    "        print(\"Processed block {0}\".format(i))\n",
    "    pickle.dump( litsearch_results, open( \"litsearch_results_{0}.p\".format(term), \"wb\" ) )\n",
    "    return summaries, pubtypes, abstracts, keywords, rcrs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed block 0\n",
      "Processed block 1\n",
      "Processed block 2\n",
      "Processed block 3\n",
      "Processed block 4\n",
      "Processed block 5\n",
      "Processed block 6\n",
      "Processed block 7\n",
      "Processed block 8\n",
      "Processed block 9\n",
      "Processed block 10\n",
      "Processed block 11\n"
     ]
    }
   ],
   "source": [
    "term = 'idiopathic pulmonary fibrosis'\n",
    "summaries, pubtypes, abstracts, keywords, rcrs = lit_search(term)\n",
    "litsearch_results = pickle.load(open(\"litsearch_results_{0}.p\".format(term), \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pmid_blocks = get_chunked_pmids(term, chunksize=200)\n",
    "pmids = list(pmid_blocks)\n",
    "\n",
    "d = {}\n",
    "\n",
    "#for i, pmid in enumerate(pmids):\n",
    "for i, summary in enumerate(summaries):\n",
    "    d[i] = pd.DataFrame(pmids[i], columns=['pmid'])\n",
    "    d[i]['title'] = d[i]['pmid'].map(lambda x: summaries[i].get(x)['title'])\n",
    "    d[i]['firstauthor'] = d[i]['pmid'].map(lambda x: summaries[i].get(x)['sortfirstauthor'])\n",
    "    d[i]['lastauthor'] = d[i]['pmid'].map(lambda x: summaries[i].get(x)['lastauthor'])\n",
    "    d[i]['journal'] = d[i]['pmid'].map(lambda x: summaries[i].get(x)['source'])\n",
    "    d[i]['pubdate'] = d[i]['pmid'].map(lambda x: summaries[i].get(x)['sortpubdate'])\n",
    "    d[i]['pubtype'] = d[i]['pmid'].map(lambda x: pubtypes[i].get(x))\n",
    "    d[i]['abstract'] = d[i]['pmid'].map(lambda x: abstracts[i].get(x))\n",
    "    d[i]['keywords'] = d[i]['pmid'].map(lambda x: keywords[i].get(x))\n",
    "    d[i]['rcr'] = d[i]['pmid'].astype(int).map(lambda x: rcrs[i].get(x))\n",
    "            \n",
    "df = pd.concat(d.values(), ignore_index=True)\n",
    "pd.set_option('max_colwidth',300)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.sort_values(by='rcr', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.firstauthor.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.lastauthor.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.journal.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.pubtype.astype(str).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df.abstract.str.contains('case-control')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.pubdate = pd.to_datetime(df.pubdate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupby(df.pubdate.map(lambda x: x.year)).pmid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.index = df.pubdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupby('pubdate').pmid.count().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
